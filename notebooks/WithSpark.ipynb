{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load as yaml_load\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pyspark.sql.functions as func\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Data cleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_config_file(config_file):\n",
    "    \"\"\"\n",
    "    Load configuration file\n",
    "    :param config_file: is the configuration file\n",
    "    :return: configuration\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    with open(config_file) as yml_config:\n",
    "        return yaml_load(yml_config)\n",
    "\n",
    "def _build_configuration(config_file):\n",
    "    \"\"\"\n",
    "    Build the operation configuration dict\n",
    "    :param config_file: is the path to the yaml config_file\n",
    "    :type: string\n",
    "    :return: config: global configuration\n",
    "    :rtype dict\n",
    "    \"\"\"\n",
    "    # yaml config\n",
    "    config = _load_config_file(config_file)\n",
    "    return config\n",
    "\n",
    "def normalize(df, columns):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    aggExpr = []\n",
    "    for column in columns:\n",
    "        aggExpr.append(mean(df[column]).alias(column))\n",
    "    averages = df.agg(*aggExpr).collect()[0]\n",
    "    selectExpr = []\n",
    "    for column in columns:\n",
    "        selectExpr.append(df[column] - averages[column])\n",
    "    return df.select(selectExpr)\n",
    "\n",
    "def get_dummies(df, list_columns):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def join_all(dfs, keys):\n",
    "        if len(dfs) > 1:\n",
    "            return dfs[0].join(join_all(dfs[1:], keys), on = keys, how = 'inner')\n",
    "        else:\n",
    "            return dfs[0]\n",
    "    dfs = []\n",
    "    combined = []\n",
    "    pivot_cols = list_columns\n",
    "    keys = df.columns\n",
    "    \n",
    "    for pivot_col in pivot_cols:\n",
    "        pivotDF =  df.groupBy(keys).pivot(pivot_col).count()\n",
    "        new_names = pivotDF.columns[:len(keys)] +  [\"{0}_{1}\".format(pivot_col, c)\\\n",
    "                                                for c in pivotDF.columns[len(keys):]] \n",
    "        df = pivotDF.toDF(*new_names).fillna(0)\n",
    "        combined.append(df)\n",
    "    df_result =  join_all(combined, keys)\n",
    "    \n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/home/ml/Documents/crimes_chigaco/config/config.yml\"\n",
    "config = _build_configuration(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_crime():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return config['connect']['PathCrimes']\n",
    "\n",
    "def path_socio():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return config['connect']['PathSocioEco']\n",
    "\n",
    "def path_columns():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return config['connect']['Pathcolumns']\n",
    "\n",
    "def path_temperature():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    return config['connect']['PathTemperature']\n",
    "\n",
    "def path_sky():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return config['connect']['PathSky']\n",
    "def spark_shape(self):\n",
    "    return (self.count(), len(self.columns))\n",
    "pyspark.sql.dataframe.DataFrame.shape = spark_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = json.loads(open(path_columns()).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_crime():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    column_name = json.loads(open(path_columns()).read())\n",
    "    df_crimes = spark.read.format(\"csv\").option(\"header\",\"true\").\\\n",
    "    option(\"mode\",\"DROPMALFORMED\").option(\"delimiter\", \";\").load(path_crime())\n",
    "    for old_name, new_name in column_name['DataCrimes'].items():\n",
    "        df_crimes = df_crimes.withColumnRenamed(old_name, new_name)\n",
    "    df_crimes = df_crimes.withColumn(\"date\", func.to_timestamp(\"date\", \"MM/dd/yyyy hh:mm:ss aaa\"))\n",
    "    \n",
    "    if config[\"List_of_crimes_prediction\"][\"with_merge\"]:\n",
    "        df_crimes = df_crimes.na.replace(config[\"List_of_crimes_prediction\"][\"to_merge\"],'primary_type')\n",
    "        return df_crimes\n",
    "\n",
    "    else:\n",
    "        return df_crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_socio():\n",
    "    \"\"\"\n",
    "\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    column_name = json.loads(open(path_columns()).read())\n",
    "    df_socio = spark.read.format(\"csv\").option(\"header\",\"true\").\\\n",
    "    option(\"mode\",\"DROPMALFORMED\").option(\"delimiter\", \",\").load(path_socio())\n",
    "    for old_name, new_name in column_name['SocioEco'].items():\n",
    "        df_socio = df_socio.withColumnRenamed(old_name, new_name)\n",
    "        \n",
    "    column_names_to_normalize = ['pct_housing_crowded','pct_households_below_poverty',  'pct_age16_unemployed' , 'pct_age25_no_highschool', 'pct_not_working_age','per_capita_income',\n",
    "                'hardship_index']\n",
    "\n",
    "    return df_socio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socio_ = df_socio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_socio_.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_nb_crimes_spark():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    df_S = df_socio()\n",
    "    df_C = df_crime()\n",
    "    df_C = df_C.filter(func.col('primary_type').isin(config['NameCrime']))\n",
    "    df_C = df_C.filter((func.col('date') > '2015-03-18') & (func.col('date') < '2017-01-15')).withColumn(\"month\", func.month(func.col(\"date\"))).withColumn(\"year\", func.year(func.col(\"date\"))).groupBy('community_area_number', 'month', 'year', 'primary_type').agg(func.count(df_C.id).alias('nb_crimes')).join(df_S, ['community_area_number'], \"inner\")\n",
    "    df_C = get_dummies(df_C , list_columns=['primary_type', 'community_area_name', 'month'])\n",
    "    return df_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_nb_crimes_spark_ = df_nb_crimes_spark()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_crimes_spark_.limit(1000).toPandas().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_temperature_spark():\n",
    "    \n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # pd.options.mode.chained_assignment = None\n",
    "    df = spark.read.format(\"csv\").option(\"header\",\"true\").\\\n",
    "    option(\"mode\",\"DROPMALFORMED\").option(\"delimiter\", \",\").load(path_temperature())\n",
    "    df = df.select('Chicago', 'datetime').withColumnRenamed('Chicago','Temperature')\n",
    "    df = df.filter((func.col('datetime') > '2017-05-10') & (func.col('datetime') < '2017-05-15'))\n",
    "    df = df.withColumn(\"month\", func.month(func.col(\"datetime\"))).\\\n",
    "    withColumn(\"year\", func.year(func.col(\"datetime\"))).withColumn(\"day\", func.dayofmonth(func.col(\"datetime\"))).\\\n",
    "    withColumn(\"hour\", func.hour(func.col(\"datetime\")))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temperature_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_sky():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.read.format(\"csv\").option(\"header\",\"true\").\\\n",
    "    option(\"mode\",\"DROPMALFORMED\").option(\"delimiter\", \",\").load(path_sky())\n",
    "    df = df.select('Chicago', 'datetime')\n",
    "    df= df.filter((func.col('datetime') > '2017-05-10') & (func.col('datetime') < '2017-05-15'))\n",
    "    df = get_dummies(df, list_columns=['Chicago'])\n",
    "    df= df.withColumn(\"month\", func.month(func.col(\"datetime\"))).\\\n",
    "    withColumn(\"year\", func.year(func.col(\"datetime\"))).withColumn(\"day\", func.dayofmonth(func.col(\"datetime\"))).\\\n",
    "    withColumn(\"hour\", func.hour(func.col(\"datetime\")))\n",
    "  \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sky_ = df_sky()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sky_.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_config_file(config_file):\n",
    "    \"\"\"\n",
    "    Load configuration file\n",
    "    :param config_file: is the configuration file\n",
    "    :return: configuration\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    with open(config_file) as yml_config:\n",
    "        return yaml_load(yml_config)\n",
    "\n",
    "def _build_configuration(config_file):\n",
    "    \"\"\"\n",
    "    Build the operation configuration dict\n",
    "    :param config_file: is the path to the yaml config_file\n",
    "    :type: string\n",
    "    :return: config: global configuration\n",
    "    :rtype dict\n",
    "    \"\"\"\n",
    "    # yaml config\n",
    "    config = _load_config_file(config_file)\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import LoadDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = \"/home/ml/Documents/crimes_chigaco_spark/config/config.yml\"\n",
    "config = _build_configuration(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "obj_df_loaded = LoadDataframe(config, '2013', '2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_crime_ = obj_df_loaded.df_crime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_crime_.limit(1000).toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socio_ = obj_df_loaded.df_socio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_socio_.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_crimes = obj_df_loaded.df_nb_crimes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_crimes.limit(1000).toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nb_crimes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_feature = ['community_area_number',\n",
    " 'month',\n",
    " 'year',\n",
    " 'primary_type',\n",
    " 'nb_crimes',\n",
    " 'community_area_name',\n",
    " 'pct_housing_crowded',\n",
    " 'pct_households_below_poverty',\n",
    " 'pct_age16_unemployed',\n",
    " 'pct_age25_no_highschool',\n",
    " 'pct_not_working_age',\n",
    " 'per_capita_income',\n",
    " 'hardship_index', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = df_nb_crimes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_nb_crimes.drop(*['community_area_number', 'month', 'year', 'primary_type', 'community_area_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_socio = ['pct_housing_crowded',\n",
    " 'pct_households_below_poverty','pct_age16_unemployed',\n",
    " 'pct_age25_no_highschool',\n",
    " 'pct_not_working_age',\n",
    " 'per_capita_income',\n",
    " 'hardship_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = data.withColumnRenamed('nb_crimes', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "for f in features_socio:\n",
    "    data = data.withColumn(f, data[f].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=features, outputCol=\"unscaled_features\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_features\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(maxIter=10, regParam=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(featuresCol=\"features\", maxIter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [vectorAssembler, standardScaler, gbt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = data.randomSplit([.7, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_result = prediction.select(*['label','prediction']).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualisation_prediction(y_test, y_pred):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    matplotlib.rc('xtick', labelsize=30) \n",
    "    matplotlib.rc('ytick', labelsize=30) \n",
    "    fig, ax = plt.subplots(figsize=(50, 40))\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(y_pred, y_test, 'ro')\n",
    "    plt.xlabel('Predicted Crime', fontsize = 30)\n",
    "    plt.ylabel('Actual Crime', fontsize = 30)\n",
    "    plt.title('Predicted Y (Crimes) to the Actual Y (Crimes)', fontsize = 30)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation_prediction(df_result['label'], df_result['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(prediction)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(df_result['label'], df_result['prediction'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
