{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load as yaml_load\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('../')\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, VectorIndexer, StandardScaler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf, udf\n",
    "from pyspark.sql.types import LongType, StringType, FloatType, IntegerType, DoubleType\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "import re\n",
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "from pyspark.ml import Pipeline\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Data cleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import LoadDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_config_file(config_file):\n",
    "    \"\"\"\n",
    "    Load configuration file\n",
    "    :param config_file: is the configuration file\n",
    "    :return: configuration\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    with open(config_file) as yml_config:\n",
    "        return yaml_load(yml_config)\n",
    "\n",
    "def _build_configuration(config_file):\n",
    "    \"\"\"\n",
    "    Build the operation configuration dict\n",
    "    :param config_file: is the path to the yaml config_file\n",
    "    :type: string\n",
    "    :return: config: global configuration\n",
    "    :rtype dict\n",
    "    \"\"\"\n",
    "    # yaml config\n",
    "    config = _load_config_file(config_file)\n",
    "    return config\n",
    "def visualisation_prediction(y_test, y_pred):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    matplotlib.rc('xtick', labelsize=30) \n",
    "    matplotlib.rc('ytick', labelsize=30) \n",
    "    fig, ax = plt.subplots(figsize=(50, 40))\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(y_pred, y_test, 'ro')\n",
    "    plt.xlabel('Predicted Crime', fontsize = 30)\n",
    "    plt.ylabel('Actual Crime', fontsize = 30)\n",
    "    plt.title('Predicted Y (Crimes) to the Actual Y (Crimes)', fontsize = 30)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    \n",
    "    \n",
    "def duration_day_func(x):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from astral import Astral\n",
    "    city_name = 'Chicago'\n",
    "    a = Astral()\n",
    "    a.solar_depression = 'civil'\n",
    "    city = a[city_name]\n",
    "    sun = city.sun(date=x, local=True)\n",
    "    return float((sun['sunset'] - sun['sunrise']).total_seconds())\n",
    "\n",
    "extract_blok = udf(lambda x : re.findall(r\"(\\w+)$\", x)[0], StringType())\n",
    "isStreet = udf(lambda x :  1 if x in ['ST', 'St', 'st'] else 0, IntegerType())\n",
    "isAV = udf(lambda x : 1 if x in ['Ave', 'AV', 'AVE'] else 0, IntegerType())\n",
    "isBLVD = udf(lambda x : 1 if x in ['BLVD'] else 0, IntegerType())\n",
    "isRD = udf(lambda x : 1 if x in ['RD'] else 0, IntegerType())\n",
    "isPL = udf(lambda x : 1 if x in ['PL', 'pl'] else 0, IntegerType())\n",
    "isBROADWAY = udf(lambda x : 1 if x in ['BROADWAY', 'Broadway'] else 0, IntegerType())\n",
    "isPKWY = udf(lambda x : 1 if x in ['PKWY', 'Pkwy'] else 0, IntegerType())\n",
    "duration_day_udf = udf(lambda x :   duration_day_func(x), FloatType())\n",
    "\n",
    "config_file = \"/home/ml/Documents/crimes_chigaco/config/config.yml\"\n",
    "config = _build_configuration(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 876 µs, sys: 0 ns, total: 876 µs\n",
      "Wall time: 940 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "obj_df_loaded = LoadDataframe(config, '2013', '2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = obj_df_loaded.df_temperature()\n",
    "df_sky  = obj_df_loaded.df_sky()\n",
    "df_socio = obj_df_loaded.df_socio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_socio = ['pct_housing_crowded',\n",
    " 'pct_households_below_poverty','pct_age16_unemployed',\n",
    " 'pct_age25_no_highschool',\n",
    " 'pct_not_working_age',\n",
    " 'per_capita_income',\n",
    " 'hardship_index']\n",
    "for f in features_socio:\n",
    "    df_socio = df_socio.withColumn(f, df_socio[f].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py:1793: UserWarning: to_replace is a dict and value is not None. value will be ignored.\n",
      "  warnings.warn(\"to_replace is a dict and value is not None. value will be ignored.\")\n"
     ]
    }
   ],
   "source": [
    "df_crime = obj_df_loaded.df_crime()\n",
    "df_crime_socio = df_crime.join(df_socio, ['community_area_number'], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_crime_socio = df_crime_socio.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio = df_crime_socio.withColumn(\"block_extract\", extract_blok(df_crime_socio.block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio = (df_crime_socio.\n",
    "                  withColumn(\"isStreet\",isStreet(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isAV\", isAV(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isBLVD\", isBLVD(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isRD\", isRD(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isPL\", isPL(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isBROADWAY\", isBROADWAY(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isPKWY\", isPKWY(df_crime_socio.block_extract)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio = df_crime_socio.withColumn('duree_day', duration_day_udf('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- community_area_number: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- cas_number: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- block: string (nullable = true)\n",
      " |-- iucr: string (nullable = true)\n",
      " |-- primary_type: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- location_description: string (nullable = true)\n",
      " |-- arrest: string (nullable = true)\n",
      " |-- domestic: string (nullable = true)\n",
      " |-- beat: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- ward: string (nullable = true)\n",
      " |-- fbi_code: string (nullable = true)\n",
      " |-- x_coordinate: string (nullable = true)\n",
      " |-- y_coordinate: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- updated_on: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- community_area_name: string (nullable = true)\n",
      " |-- pct_housing_crowded: float (nullable = true)\n",
      " |-- pct_households_below_poverty: float (nullable = true)\n",
      " |-- pct_age16_unemployed: float (nullable = true)\n",
      " |-- pct_age25_no_highschool: float (nullable = true)\n",
      " |-- pct_not_working_age: float (nullable = true)\n",
      " |-- per_capita_income: float (nullable = true)\n",
      " |-- hardship_index: float (nullable = true)\n",
      " |-- block_extract: string (nullable = true)\n",
      " |-- isStreet: integer (nullable = true)\n",
      " |-- isAV: integer (nullable = true)\n",
      " |-- isBLVD: integer (nullable = true)\n",
      " |-- isRD: integer (nullable = true)\n",
      " |-- isPL: integer (nullable = true)\n",
      " |-- isBROADWAY: integer (nullable = true)\n",
      " |-- isPKWY: integer (nullable = true)\n",
      " |-- duree_day: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_crime_socio.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio = (\n",
    "df_crime_socio.withColumn(\"month\", func.month(func.col(\"date\"))).\n",
    "withColumn(\"year\", func.year(func.col(\"date\"))).\n",
    "withColumn(\"day\", func.dayofmonth(func.col(\"date\"))).\n",
    "withColumn(\"hour\", func.hour(func.col(\"date\"))).withColumn(\"minute\", func.minute(func.col(\"date\"))).\n",
    "withColumn(\"dayofmonth\", func.dayofmonth(func.col(\"date\"))).   \n",
    "withColumn(\"dayofyear\", func.dayofyear(func.col(\"date\"))).\n",
    "withColumn(\"dayofweek\", func.dayofweek(func.col(\"date\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.drop('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_crime_socio.join(df_temp, ['year', 'month','day','hour'], how = \"inner\").\\\n",
    "                        join(df_sky, ['year', 'month','day','hour'], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.withColumn('latitude', df_total['latitude'].cast(FloatType()))\n",
    "df_total = df_total.withColumn('longitude', df_total['longitude'].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_crimes = config[\"List_of_crimes_prediction\"][\"with_merge_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_total.where(col('primary_type').isin(list_of_crimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = ['domestic']\n",
    "numericCols = ['year', 'month', 'day', 'hour','minute', 'latitude',\n",
    "               'longitude', 'isStreet', 'isAV',\n",
    "               'isBLVD', 'isRD', 'isPL', 'isBROADWAY',\n",
    "               'isPKWY', 'duree_day', 'minute','dayofmonth',\n",
    "               'dayofyear','dayofweek','Temperature',\n",
    "               'pct_housing_crowded',\n",
    "               'pct_households_below_poverty',\n",
    "               'pct_age16_unemployed',\n",
    "               'pct_age25_no_highschool',\n",
    "               'pct_not_working_age',\n",
    "               'per_capita_income',\n",
    "               'hardship_index',\n",
    "                'Chicago_broken clouds',\n",
    "                'Chicago_drizzle',\n",
    "                 'Chicago_few clouds',\n",
    "                'Chicago_fog',\n",
    "                 'Chicago_haze',\n",
    "                 'Chicago_heavy intensity drizzle',\n",
    "                 'Chicago_heavy intensity rain',\n",
    "                 'Chicago_heavy snow',\n",
    "                 'Chicago_light intensity drizzle',\n",
    "                 'Chicago_light rain',\n",
    "                 'Chicago_light rain and snow',\n",
    "                 'Chicago_light snow',\n",
    "                 'Chicago_mist',\n",
    "                 'Chicago_moderate rain',\n",
    "                 'Chicago_overcast clouds',\n",
    "                 'Chicago_proximity thunderstorm',\n",
    "                 'Chicago_scattered clouds',\n",
    "                 'Chicago_sky is clear',\n",
    "                 'Chicago_snow',\n",
    "                 'Chicago_thunderstorm',\n",
    "                 'Chicago_thunderstorm with heavy rain',\n",
    "                 'Chicago_thunderstorm with light rain',\n",
    "                 'Chicago_thunderstorm with rain',\n",
    "                'Chicago_very heavy rain'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='label', featuresCol='features',numTrees=150,  maxDepth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = []\n",
    "label_stringIdx = StringIndexer(inputCol = 'primary_type', outputCol = 'label')\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages.append(stringIndexer)\n",
    "    stages.append(encoder)\n",
    "stages.append(label_stringIdx)\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages.append(assembler)\n",
    "stages.append(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_and_model = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "pipeline_model = pipeline_and_model.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test_pred = pipeline_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = df_test_pred.select(col(\"label\"), col(\"prediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions_ = predictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracy = evaluator.evaluate(predictions_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
