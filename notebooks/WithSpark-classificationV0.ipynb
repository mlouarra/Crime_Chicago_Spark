{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import load as yaml_load\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('../')\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, VectorIndexer, StandardScaler\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf, udf\n",
    "from pyspark.sql.types import LongType, StringType, FloatType, IntegerType, DoubleType\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "import re\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"Data cleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_dataset import LoadDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_config_file(config_file):\n",
    "    \"\"\"\n",
    "    Load configuration file\n",
    "    :param config_file: is the configuration file\n",
    "    :return: configuration\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    with open(config_file) as yml_config:\n",
    "        return yaml_load(yml_config)\n",
    "\n",
    "def _build_configuration(config_file):\n",
    "    \"\"\"\n",
    "    Build the operation configuration dict\n",
    "    :param config_file: is the path to the yaml config_file\n",
    "    :type: string\n",
    "    :return: config: global configuration\n",
    "    :rtype dict\n",
    "    \"\"\"\n",
    "    # yaml config\n",
    "    config = _load_config_file(config_file)\n",
    "    return config\n",
    "def visualisation_prediction(y_test, y_pred):\n",
    "    import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    matplotlib.rc('xtick', labelsize=30) \n",
    "    matplotlib.rc('ytick', labelsize=30) \n",
    "    fig, ax = plt.subplots(figsize=(50, 40))\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(y_pred, y_test, 'ro')\n",
    "    plt.xlabel('Predicted Crime', fontsize = 30)\n",
    "    plt.ylabel('Actual Crime', fontsize = 30)\n",
    "    plt.title('Predicted Y (Crimes) to the Actual Y (Crimes)', fontsize = 30)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\n",
    "    \n",
    "    \n",
    "def duration_day_func(x):\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from astral import Astral\n",
    "    city_name = 'Chicago'\n",
    "    a = Astral()\n",
    "    a.solar_depression = 'civil'\n",
    "    city = a[city_name]\n",
    "    sun = city.sun(date=x, local=True)\n",
    "    return float((sun['sunset'] - sun['sunrise']).total_seconds())\n",
    "\n",
    "extract_blok = udf(lambda x : re.findall(r\"(\\w+)$\", x)[0], StringType())\n",
    "isStreet = udf(lambda x :  1 if x in ['ST', 'St', 'st'] else 0, IntegerType())\n",
    "isAV = udf(lambda x : 1 if x in ['Ave', 'AV', 'AVE'] else 0, IntegerType())\n",
    "isBLVD = udf(lambda x : 1 if x in ['BLVD'] else 0, IntegerType())\n",
    "isRD = udf(lambda x : 1 if x in ['RD'] else 0, IntegerType())\n",
    "isPL = udf(lambda x : 1 if x in ['PL', 'pl'] else 0, IntegerType())\n",
    "isBROADWAY = udf(lambda x : 1 if x in ['BROADWAY', 'Broadway'] else 0, IntegerType())\n",
    "isPKWY = udf(lambda x : 1 if x in ['PKWY', 'Pkwy'] else 0, IntegerType())\n",
    "duration_day_udf = udf(lambda x :   duration_day_func(x), FloatType())\n",
    "\n",
    "config_file = \"/home/ml/Documents/crimes_chigaco/config/config.yml\"\n",
    "config = _build_configuration(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "obj_df_loaded = LoadDataframe(config, '2013', '2014')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = obj_df_loaded.df_temperature()\n",
    "df_sky  = obj_df_loaded.df_sky()\n",
    "df_socio = obj_df_loaded.df_socio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col, isnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_socio = ['pct_housing_crowded',\n",
    " 'pct_households_below_poverty','pct_age16_unemployed',\n",
    " 'pct_age25_no_highschool',\n",
    " 'pct_not_working_age',\n",
    " 'per_capita_income',\n",
    " 'hardship_index']\n",
    "for f in features_socio:\n",
    "    df_socio = df_socio.withColumn(f, df_socio[f].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime = obj_df_loaded.df_crime()\n",
    "df_crime_socio = df_crime.join(df_socio, ['community_area_number'], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_crime_socio = df_crime_socio.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio = df_crime_socio.withColumn(\"block_extract\", extract_blok(df_crime_socio.block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio = (df_crime_socio.\n",
    "                  withColumn(\"isStreet\",isStreet(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isAV\", isAV(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isBLVD\", isBLVD(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isRD\", isRD(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isPL\", isPL(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isBROADWAY\", isBROADWAY(df_crime_socio.block_extract)).\n",
    "                  withColumn(\"isPKWY\", isPKWY(df_crime_socio.block_extract)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio = df_crime_socio.withColumn('duree_day', duration_day_udf('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio.limit(1000).toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio = (\n",
    "df_crime_socio.withColumn(\"month\", func.month(func.col(\"date\"))).\n",
    "withColumn(\"year\", func.year(func.col(\"date\"))).\n",
    "withColumn(\"day\", func.dayofmonth(func.col(\"date\"))).\n",
    "withColumn(\"hour\", func.hour(func.col(\"date\"))).withColumn(\"minute\", func.minute(func.col(\"date\"))).\n",
    "withColumn(\"dayofmonth\", func.dayofmonth(func.col(\"date\"))).   \n",
    "withColumn(\"dayofyear\", func.dayofyear(func.col(\"date\"))).\n",
    "withColumn(\"dayofweek\", func.dayofweek(func.col(\"date\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crime_socio.limit(100).toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_crime_socio.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_temp.drop('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_crime_socio.join(df_temp, ['year', 'month','day','hour'], how = \"inner\").\\\n",
    "                        join(df_sky, ['year', 'month','day','hour'], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_total.limit(1000).toPandas().sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = df_total.withColumn('latitude', df_total['latitude'].cast(FloatType()))\n",
    "df_total = df_total.withColumn('longitude', df_total['longitude'].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_crimes = config[\"List_of_crimes_prediction\"][\"with_merge_pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_total.where(col('primary_type').isin(list_of_crimes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricalColumns = ['domestic']\n",
    "numericCols = ['year', 'month', 'day', 'hour','minute', 'latitude',\n",
    "               'longitude', 'isStreet', 'isAV',\n",
    "               'isBLVD', 'isRD', 'isPL', 'isBROADWAY',\n",
    "               'isPKWY', 'duree_day', 'minute','dayofmonth',\n",
    "               'dayofyear','dayofweek','Temperature',\n",
    "               'pct_housing_crowded',\n",
    "               'pct_households_below_poverty',\n",
    "               'pct_age16_unemployed',\n",
    "               'pct_age25_no_highschool',\n",
    "               'pct_not_working_age',\n",
    "               'per_capita_income',\n",
    "               'hardship_index',\n",
    "                'Chicago_broken clouds',\n",
    "                'Chicago_drizzle',\n",
    "                 'Chicago_few clouds',\n",
    "                'Chicago_fog',\n",
    "                 'Chicago_haze',\n",
    "                 'Chicago_heavy intensity drizzle',\n",
    "                 'Chicago_heavy intensity rain',\n",
    "                 'Chicago_heavy snow',\n",
    "                 'Chicago_light intensity drizzle',\n",
    "                 'Chicago_light rain',\n",
    "                 'Chicago_light rain and snow',\n",
    "                 'Chicago_light snow',\n",
    "                 'Chicago_mist',\n",
    "                 'Chicago_moderate rain',\n",
    "                 'Chicago_overcast clouds',\n",
    "                 'Chicago_proximity thunderstorm',\n",
    "                 'Chicago_scattered clouds',\n",
    "                 'Chicago_sky is clear',\n",
    "                 'Chicago_snow',\n",
    "                 'Chicago_thunderstorm',\n",
    "                 'Chicago_thunderstorm with heavy rain',\n",
    "                 'Chicago_thunderstorm with light rain',\n",
    "                 'Chicago_thunderstorm with rain',\n",
    "                'Chicago_very heavy rain'\n",
    "]\n",
    "stages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "rf = RandomForestClassifier(labelCol= \"label\", featuresCol=\"features\", numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stringIdx = StringIndexer(inputCol = 'primary_type', outputCol = 'label')\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages += [assembler] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages = stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipelineModel = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prepared  = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedCols = ['label', 'features'] + categoricalColumns + numericCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.select(selectedCols)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(df.take(5), columns=df.columns).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
